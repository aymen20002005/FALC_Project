{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Tf72n1TT9aGaACfaea7Okql-qaCzMKpV","timestamp":1676207162370}],"mount_file_id":"1OhWAo0AKDT9BW6Go7HVgy7j3ZA2rmFCJ","authorship_tag":"ABX9TyOsEqOUZfXEiGUBggf4A/kK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QzsRzf_au2ID"},"outputs":[],"source":["#Run only 1 time for every new session\n","!apt update\n","!apt install enchant --fix-missing\n","!apt install -qq enchant\n","!pip install pyenchant\n","!apt-get install myspell-fr\n","!pip install transformers\n","!pip install mtranslate\n"]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","import numpy as np\n","import enchant\n","import requests\n","import json\n","import concurrent.futures\n","import math\n","import nltk\n","\n","dictionary = enchant.Dict(\"fr\")\n","word_freqs = Counter()\n","current_size = 0\n","\n","# Set maximum size max is 10/12 gb (collab limitation)\n","max_size = 9 * 1024 * 1024 * 1024\n","\n","# Define the number of articles you want to retrieve (>2000 for decent results)\n","n_articles = 50\n","\n","urls = [] \n","def fetch_random_article():\n","    response = requests.get(\"https://fr.wikipedia.org/api/rest_v1/page/random/summary\")\n","    data = json.loads(response.text)\n","    url = data[\"content_urls\"][\"desktop\"][\"page\"]\n","    return url\n","\n","with concurrent.futures.ThreadPoolExecutor() as executor:\n","    future_to_url = {executor.submit(fetch_random_article): i for i in range(n_articles)}\n","    for future in concurrent.futures.as_completed(future_to_url):\n","        url = future.result()\n","        urls.append(url)\n","\n","for url in urls:\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    text = soup.get_text()\n","    text = ''.join([i for i in text if not i.isdigit()])\n","    words = text.split()\n","    for word in words:\n","        if dictionary.check(word):\n","            word_freqs[word] += 1\n","    current_size += len(text)\n","    if current_size >= max_size:\n","        break\n","\n","# Zipf's law\n","if len(word_freqs) > 1:\n","    ranks = range(1, len(word_freqs) + 1)\n","    freqs = [count for _, count in word_freqs.items()]\n","    log_ranks = np.log(ranks)\n","    log_freqs = np.log(freqs)\n","    a, b = np.polyfit(log_ranks, log_freqs, 1)\n","else:\n","    a, b = None, None\n","\n","# Identify simple words\n","simple_words = set()\n","word_freqs_list = word_freqs.most_common()\n","threshold_frequency = 7 # Set a threshold frequency to define what constitutes a simple word\n","for word, freq in word_freqs_list:\n","    if freq > threshold_frequency:\n","        simple_words.add(word)"],"metadata":{"id":"Mu-fNRudvEcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import nltk\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.cluster import KMeans\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n","model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n","nltk.download('punkt')\n","def extract_phrases(text):\n","    phrases = nltk.sent_tokenize(text)\n","    simple_phrases = []\n","    for phrase in phrases:\n","        words = phrase.split()\n","        simple = True\n","        for word in words:\n","            if not dictionary.check(word):\n","                simple = False\n","                break\n","        if simple:\n","            simple_phrases.append(phrase)\n","    return simple_phrases\n","\n","def encode_phrase(phrase):\n","    input_ids = tokenizer.encode(phrase, return_tensors='pt')\n","    with torch.no_grad():\n","        encoded_phrase = model(input_ids).last_hidden_state[0][0].numpy()\n","    return encoded_phrase\n","\n","simple_phrases = []\n","for url in urls:\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    text = soup.get_text()\n","    text = ''.join([i for i in text if not i.isdigit()])\n","    simple_phrases += extract_phrases(text)\n","\n","encoded_phrases = []\n","for phrase in simple_phrases:\n","    encoded_phrase = encode_phrase(phrase)\n","    encoded_phrases.append(encoded_phrase)\n","\n","\n","encoded_phrases = np.array(encoded_phrases)\n","\n","kmeans = KMeans(n_clusters=30)\n","kmeans.fit(encoded_phrases)\n","\n","cluster_labels = kmeans.labels_\n","\n","\n","# Save the simplest phrases to a file\n","with open('/content/drive/MyDrive/Colab Notebooks/output.txt', 'w') as f:\n","    for cluster_label in set(cluster_labels):\n","        cluster = np.where(cluster_labels == cluster_label)[0]\n","        for i in cluster:\n","            f.write(f'{simple_phrases[i]}\\n')"],"metadata":{"id":"u3HVRygPwuAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["min_words = 3\n","max_words = 8\n","\n","# Read the file\n","with open(\"/content/drive/MyDrive/Colab Notebooks/output.txt\", \"r\") as file:\n","    lines = file.readlines()\n","\n","# Remove duplicates and limit the number of words\n","lines = list(set(line.strip() for line in lines if len(line.strip().split()) >= min_words and len(line.strip().split()) <= max_words))\n","\n","# Write the cleaned sentences back to the file\n","with open(\"/content/drive/MyDrive/Colab Notebooks/output.txt\", \"w\") as file:\n","    file.writelines(\"\\n\".join(lines))\n"],"metadata":{"id":"xF0LXdZIqYEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mtranslate import translate\n","with open('/content/drive/MyDrive/Colab Notebooks/output.txt', 'r') as f:\n","    with open('/content/drive/MyDrive/Colab Notebooks/output2.txt', 'w') as out:\n","        for line in f:\n","            translated_line = translate(line.strip(),\"english\")  \n","            out.write(translated_line + '\\n')"],"metadata":{"id":"krTgylG6sdmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","def check_grammar(sentence):\n","    doc = nlp(sentence)\n","\n","    \n","    # Check for incomplete or fragmented sentences\n","    if len(doc) < 2:\n","        return \"Incomplete sentence\"\n","    for sent in doc.sents:\n","        if len(sent) < 2:\n","            return \"Fragmented sentence\"\n","    \n","    # Check for missing subjects\n","    for token in doc:\n","        if token.dep_ == 'nsubj' and not token.is_punct:\n","\n","            break\n","    else:\n","        return \"Missing subject\"\n","    \n","    # Check for missing verbs\n","    for token in doc:\n","        if token.pos_ == 'VERB' and not token.is_punct:\n","            break\n","    else:\n","        return \"Missing verb\"\n","    \n","    # Check for other grammatical issues\n","    for token in doc:\n","        if token.pos_ == 'VERB' and token.dep_ == 'aux' and not token.is_punct:\n","            return \"Auxiliary verb should not be used\"\n","        if token.tag_ in ['VBD', 'VBN'] and not token.is_punct:\n","            return \"Use present tense instead of past participle or past tense\"\n","        if token.dep_ == 'nsubjpass' and not token.is_punct:\n","            return \"Passive voice should not be used\"\n","        if token.tag_ == 'NEG' and not token.is_punct:\n","            return \"Negative sentence\"\n","    \n","    # If no grammatical issues found, return None\n","    return None\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/output2.txt', 'r') as input_file, open('/content/drive/MyDrive/Colab Notebooks/output3.txt', 'w') as output_file:\n","    # Iterate through the lines in the input file\n","    for line in input_file:\n","        # Parse the line into sentences using spaCy\n","        doc = nlp(line)\n","        for sent in doc.sents:\n","            # Check the grammar of the sentence\n","            result = check_grammar(sent.text)\n","            # If the sentence is correct, save it in the output file\n","            if not result:\n","                output_file.write(sent.text + '\\n')\n"],"metadata":{"id":"IcbBE5EQviba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mtranslate import translate\n","with open('/content/drive/MyDrive/Colab Notebooks/output3.txt', 'r') as f:\n","    with open('/content/drive/MyDrive/Colab Notebooks/output4.txt', 'w') as out:\n","        for line in f:\n","            translated_line = translate(line.strip(),\"fr\",\"en\")  \n","            out.write(translated_line + '\\n')\n","min_words = 3\n","max_words = 6\n","\n","# Read the file\n","with open(\"/content/drive/MyDrive/Colab Notebooks/output4.txt\", \"r\") as file:\n","    lines = file.readlines()\n","\n","# Remove duplicates and limit the number of words\n","lines = list(set(line.strip() for line in lines if len(line.strip().split()) >= min_words and len(line.strip().split()) <= max_words))\n","\n","# Write the cleaned sentences back to the file\n","with open(\"/content/drive/MyDrive/Colab Notebooks/output4.txt\", \"w\") as file:\n","    file.writelines(\"\\n\".join(lines))"],"metadata":{"id":"hx8hkqz921dv"},"execution_count":null,"outputs":[]}]}