{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Afin de procéder au test de ce code, il convient préalablement de se munir d'un jeu de données composé de phrases FALC et complexes, formaté en xlsx, et d'adapter l'ensemble des chemins de fichier employés dans ce code en fonction de l'environnement et de l'emplacement du nouveau fichier dataset.xlsx.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XYSQ1_xUXfD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CI5Pz2DHiv8"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjgIv7F9mVow"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the xlsx file\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/dataset.xlsx', usecols=['A', 'B'])\n",
        "data = data[data['B'].str.count('\\n') == 0]\n",
        "# Save simple and complex sentences as separate text files\n",
        "data['A'].to_csv('/content/drive/MyDrive/Colab Notebooks/model/simple_sentences.txt', index=False, header=None)\n",
        "data['B'].to_csv('/content/drive/MyDrive/Colab Notebooks/model/complex_sentences.txt', index=False, header=None)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/model/complex_sentences.txt', 'r') as f:\n",
        "    complex_sentences = f.read().splitlines()\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/model/complex_sentences.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = text.replace('\"', '')\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/model/complex_sentences.txt', 'w') as file:\n",
        "    file.write(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVyID8wt3_md"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7SFlgbL1YE-"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def read_sentences(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        sentences = [line.strip() for line in f.readlines()]\n",
        "    return sentences\n",
        "\n",
        "simple_sentences = read_sentences(\"/content/drive/MyDrive/Colab Notebooks/model/simple_sentences.txt\")\n",
        "complex_sentences = read_sentences(\"/content/drive/MyDrive/Colab Notebooks/model/complex_sentences.txt\")\n",
        "\n",
        "data_dict = {'simple': simple_sentences, 'complex': complex_sentences}\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['test']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP6cNMkJ2aAG"
      },
      "outputs": [],
      "source": [
        "def preprocess(examples):\n",
        "    sources = examples['complex']\n",
        "    targets = examples['simple']\n",
        "\n",
        "    source_tokenized = tokenizer([\"simplifier: \" + src for src in sources], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    target_tokenized = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": source_tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": source_tokenized[\"attention_mask\"],\n",
        "        \"labels\": target_tokenized[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"simple\", \"complex\"])\n",
        "val_dataset = val_dataset.map(preprocess, batched=True, remove_columns=[\"simple\", \"complex\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "G_zaDA-u1aFU",
        "outputId": "f99e2184-c7d7-4b26-ed58-c2c725ae26ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5904' max='5904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5904/5904 09:51, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.116753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.125400</td>\n",
              "      <td>0.108213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.118600</td>\n",
              "      <td>0.103210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.111000</td>\n",
              "      <td>0.100984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>0.099048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.102500</td>\n",
              "      <td>0.097438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.099200</td>\n",
              "      <td>0.096148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.096400</td>\n",
              "      <td>0.095683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.091700</td>\n",
              "      <td>0.095139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.094895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.089600</td>\n",
              "      <td>0.094726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.092400</td>\n",
              "      <td>0.094617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small/spiece.model',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=12,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    learning_rate= 5e-5,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIWNgbFPoeOa"
      },
      "source": [
        "**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LXQtDECS4Z9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cd94a3-4151-4cfa-ecae-8402e9c6e70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le nouveau logiciel de gestion du projet nous retarde beaucoup.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model/fine_tuned_t5_small\")\n",
        "\n",
        "input_text = \"Le nouveau logiciel de gestion de projet que nous avons acheté est très complexe et difficile à utiliser, ce qui a entraîné de nombreux retards dans la réalisation de nos tâches.\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "summary_ids = model.generate(input_ids=input_ids,\n",
        "                              num_beams=4,\n",
        "                              max_length=60,\n",
        "                              repetition_penalty=2.5,\n",
        "                              length_penalty=1.0)\n",
        "\n",
        "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(summary_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}